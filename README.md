# Gradient-Descent-from-Scratch-Linear-Regression-in-Python
# ğŸ“‰ - Descente du Gradient

Dans ce mini-projet, nous souhaitons prÃ©dire le **bÃ©nÃ©fice** d'une entreprise en fonction de la **population** d'une ville Ã  l'aide de la **rÃ©gression linÃ©aire**. Nous allons implÃ©menter **l'algorithme de descente du gradient** et l'appliquer sur deux jeux de donnÃ©es :
- UnivariÃ© : population â†’ bÃ©nÃ©fice
- MultivariÃ© : superficie, nombre de chambres â†’ prix d'une maison

---

## ğŸ“ Fichiers

- `data.csv` : donnÃ©es avec population et bÃ©nÃ©fice (97 exemples)
- `dataMulti.csv` : donnÃ©es avec superficie, nombre de chambres, et prix (47 exemples)
- `gradient_descent.py` (ou notebook `.ipynb`) : implÃ©mentation complÃ¨te

---

## ğŸ§® Objectifs pÃ©dagogiques

- ImplÃ©menter la **fonction de coÃ»t** pour la rÃ©gression linÃ©aire
- ImplÃ©menter la **descente du gradient** pour optimiser les paramÃ¨tres `theta`
- Visualiser les rÃ©sultats : ligne de rÃ©gression, courbe de coÃ»t
- Normaliser les donnÃ©es dans le cas multivariÃ©
- Comparer avec `scikit-learn`

---

## ğŸ“Š Partie 1 - RÃ©gression LinÃ©aire Simple (1 variable)

**DonnÃ©es** : Population d'une ville (en milliers) vs BÃ©nÃ©fice (en milliers)

### ğŸ”§ Ã‰tapes :

1. Chargement des donnÃ©es
2. Visualisation des donnÃ©es (`scatter plot`)
3. ImplÃ©mentation de la **fonction de coÃ»t** `computeCost`
4. ImplÃ©mentation de lâ€™**algorithme de descente du gradient**
5. Affichage :
   - de la **fonction hypothÃ¨se**
   - du **coÃ»t par itÃ©ration**
6. PrÃ©dictions pour :
   - une population de 35 000
   - une population de 70 000

---

## ğŸ“ˆ Partie 2 - RÃ©gression LinÃ©aire Multiple (2 variables)

**DonnÃ©es** : Surface (ftÂ²), Nombre de chambres â†’ Prix (en $)

### ğŸ”§ Ã‰tapes :

1. Chargement du fichier `dataMulti.csv`
2. CrÃ©ation des matrices X et y
3. **Normalisation des features**
4. Application de la descente du gradient
5. Comparaison **avec et sans normalisation**
6. **VÃ©rification avec `scikit-learn`**

---

## âš™ï¸ Fonctions implÃ©mentÃ©es

- `computeCost(X, y, theta)` : calcule le coÃ»t de la fonction hypothÃ¨se
- `gradientDescent(X, y, theta, alpha, iterations)` : optimise `theta`
- `featureNormalization(X)` : normalise les donnÃ©es pour la rÃ©gression multiple

---

## ğŸ§  Renforcement de lâ€™apprentissage

IdÃ©es supplÃ©mentaires pour approfondir :
- Ajouter la **visualisation du coÃ»t en 3D** (surface ou contour)
- Comparer la vitesse de convergence avec diffÃ©rents `alpha`
- Appliquer un **early stopping** si le coÃ»t stagne
- Essayer d'autres techniques comme la **descente stochastique**

---

## âœ… RÃ©sultats attendus

- RÃ©duction du coÃ»t Ã  chaque itÃ©ration
- Alignement de la ligne de rÃ©gression sur les donnÃ©es
- PrÃ©dictions cohÃ©rentes
- Convergence plus rapide avec normalisation

---

## ğŸ“š BibliothÃ¨ques utilisÃ©es

- `numpy` : calculs matriciels
- `matplotlib` : visualisation
- `scikit-learn` (pour la comparaison finale)

---



